{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Background\n",
    "\n",
    "*Wik Hung Pun*\n",
    "\n",
    "*6-9-2017*\n",
    "\n",
    "Collaborative filtering (CF) is a topic that eluded me in my quest of studying machine learning in the past. This steam games dataset gave me a reason to explore and learn more about CF. After reading some excellent works written by [Ethan Rosenthal](http://blog.ethanrosenthal.com/), [Katherine Bailey](http://katbailey.github.io/post/matrix-factorization-with-tensorflow/), and [Jesse Steinweg-Woods](https://jessesw.com/Rec-System/), I have to say the concept behind collaborative filtering is fairly simple and easy to understand. Although I do not think I can explain the concept nearly as well as these authors, I still would like to share what I have learned with you as a primer for learning collaborative filtering (and reinforce my own learning). For those of you interested in the topic, please do check out the authors I have linked. Now, without further ado..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction\n",
    "Collaborative filtering (CF) is a technique used for being recommender systems. The goal of CF is to infer the preferences for new items given the known preferences from all the users. [Rosenthal](http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/) has a great post explaining and implementing item- and user-based CF systems. I'd highly recommend you to read it if you are interested in recommender systems.\n",
    "\n",
    "What I would like to focus on is a technique calls **matrix factorization**. [There are a lot of math-y stuff behind it](https://en.wikipedia.org/wiki/Matrix_decomposition), but, for the purpose of this recommender system, we can simply think of it as solving a matrix algebra question via [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication).\n",
    "\n",
    "In this case, if we visualize the transactions as a big matrix, where users are the rows and games are the columns. This big matrix can, in turn, be decomposed into two matrices with dimensions of *users x features* (U), and *features x games* (V). The Steinweg-Woods's image gives a clear depiction of this idea. ![id](https://jessesw.com/images/Rec_images/ALS_Image_Test.png) Once we have the U and V matrices estimated, we can then take the dot product of the two to find the predicted game preferences for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explicit vs Implicit CF\n",
    "Now you have learned basic concept about CF, it is important to point out there are two types of CF: **explicit** and **implicit**. In explicit CF, the values we fill in the users by items matrix were preferences collected *explictly* from users (e.g., thumbs up/down, likes, user ratings, etc.). In contrast, we do not have these direct indicators of preferences from users with implicit CF. Instead, we only have *indirect* indicators such as whether they purchased the product or used the product. For instance, we only know a user bought a game in this dataset and the person might have even played it for a few hours, but we do not know if the user actually liked the game. For all we know, the user may hate the game after playing it! With implicit CF, we try to take account of these possible variations in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Miscellaneous\n",
    "Prior to actually getting to the code, it'd be remiss of me to not mention there are actually great packages available of conducting CF analysis. [Rosenthal](http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/) discussed the strengthes and weaknesses of some of these packages. Since these packages are not available on Kaggle (as fas as I know), I implemented my own here, but you should look for others' implementation if you are working outside of the Kaggle environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Code Stuff\n",
    "Import packages. Exciting stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Taking a look at the dataset. I named the last column as not needed since it does not appear to be associated with anything. The thing to look out for in this dataset is that the purchases and plays are separated into two separate rows. For the purpose of the analysis I will have to convert these into one record. I convert the dataset with following rules:\n",
    "1. If a game is purchased but never played, hours played = 0\n",
    "2. If a game is purchased *and* played, I keep the hours played and remove the purchase record (Playing the game implies it was purchased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Game</th>\n",
       "      <th>Action</th>\n",
       "      <th>Hours</th>\n",
       "      <th>Not Needed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151603712</td>\n",
       "      <td>The Elder Scrolls V Skyrim</td>\n",
       "      <td>purchase</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151603712</td>\n",
       "      <td>The Elder Scrolls V Skyrim</td>\n",
       "      <td>play</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151603712</td>\n",
       "      <td>Fallout 4</td>\n",
       "      <td>purchase</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151603712</td>\n",
       "      <td>Fallout 4</td>\n",
       "      <td>play</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151603712</td>\n",
       "      <td>Spore</td>\n",
       "      <td>purchase</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserID                        Game    Action  Hours  Not Needed\n",
       "0  151603712  The Elder Scrolls V Skyrim  purchase    1.0           0\n",
       "1  151603712  The Elder Scrolls V Skyrim      play  273.0           0\n",
       "2  151603712                   Fallout 4  purchase    1.0           0\n",
       "3  151603712                   Fallout 4      play   87.0           0\n",
       "4  151603712                       Spore  purchase    1.0           0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '../input/steam-200k.csv'\n",
    "path = 'steam-200k.csv'\n",
    "df = pd.read_csv(path, header = None,\n",
    "                 names = ['UserID', 'Game', 'Action', 'Hours', 'Not Needed'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating a new variable 'Hours Played' and code it as previously described.\n",
    "df['Hours_Played'] = df['Hours'].astype('float32')\n",
    "\n",
    "df.loc[(df['Action'] == 'purchase') & (df['Hours'] == 1.0), 'Hours_Played'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Game</th>\n",
       "      <th>Hours_Played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65430</th>\n",
       "      <td>5250</td>\n",
       "      <td>Alien Swarm</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65424</th>\n",
       "      <td>5250</td>\n",
       "      <td>Cities Skylines</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65435</th>\n",
       "      <td>5250</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65436</th>\n",
       "      <td>5250</td>\n",
       "      <td>Counter-Strike Source</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65437</th>\n",
       "      <td>5250</td>\n",
       "      <td>Day of Defeat</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserID                   Game  Hours_Played\n",
       "65430    5250            Alien Swarm           4.9\n",
       "65424    5250        Cities Skylines         144.0\n",
       "65435    5250         Counter-Strike           0.0\n",
       "65436    5250  Counter-Strike Source           0.0\n",
       "65437    5250          Day of Defeat           0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the df by User ID, games, and hours played\n",
    "# Drop the duplicated records, and unnecessary columns\n",
    "df.UserID = df.UserID.astype('int')\n",
    "df = df.sort_values(['UserID', 'Game', 'Hours_Played'])\n",
    "\n",
    "clean_df = df.drop_duplicates(['UserID', 'Game'], keep = 'last').drop(['Action', 'Hours', 'Not Needed'], axis = 1)\n",
    "\n",
    "# every transaction is represented by only one record now\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12393 users and 5155 games in the data\n"
     ]
    }
   ],
   "source": [
    "n_users = len(clean_df.UserID.unique())\n",
    "n_games = len(clean_df.Game.unique())\n",
    "\n",
    "print('There are {0} users and {1} games in the data'.format(n_users, n_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20% of the user-item matrix is filled\n"
     ]
    }
   ],
   "source": [
    "# If we build a matrix of users x games, how many cells in the matrix will be filled?\n",
    "sparsity = clean_df.shape[0] / float(n_users * n_games)\n",
    "print('{:.2%} of the user-item matrix is filled'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here \n",
    "user_counter = Counter()\n",
    "for user in clean_df.UserID.tolist():\n",
    "    user_counter[user] +=1\n",
    "\n",
    "game_counter = Counter()\n",
    "for game in clean_df.Game.tolist():\n",
    "    game_counter[game] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the dictionaries to convert user and games to idx and back\n",
    "user2idx = {user: i for i, user in enumerate(clean_df.UserID.unique())}\n",
    "idx2user = {i: user for user, i in user2idx.items()}\n",
    "\n",
    "game2idx = {game: i for i, game in enumerate(clean_df.Game.unique())}\n",
    "idx2game = {i: game for game, i in game2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the user and games to idx\n",
    "user_idx = clean_df['UserID'].apply(lambda x: user2idx[x]).values\n",
    "game_idx = clean_df['gameIdx'] = clean_df['Game'].apply(lambda x: game2idx[x]).values\n",
    "hours = clean_df['Hours_Played'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Converting the data to a user x game matrix\n",
    "Since we do not have explicit indicators in this dataset, we fill the users by games matrix with simple preferred (1) or not (0). This (preference) matrix  indicates that the user purchased and/or played the game with a 1, whereas 0 means no interaction between the user and the game. \n",
    "\n",
    "Now, one obvious quesiton is what if the users dislike their purchases? If all purchases are represented by ones, how do we find out the ones that users actually regret buying? Well, we handle this situation by constructing a **confidence matrix**. This confidence matrix has the same dimension as the preference matrix and is populated with __*hours played*__. Intuitively, it means that the more time a user spent playing the game, we have more *confidence* in that the user actually *liked/preferred* the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using a sparse matrix will be more memory efficient and necessary for larger dataset, \n",
    "# but this works for now.\n",
    "\n",
    "zero_matrix = np.zeros(shape = (n_users, n_games)) # Create a zero matrix\n",
    "user_game_pref = zero_matrix.copy()\n",
    "user_game_pref[user_idx, game_idx] = 1 # Fill the matrix will preferences (bought)\n",
    "\n",
    "user_game_interactions = zero_matrix.copy()\n",
    "# Fill the confidence with (hours played)\n",
    "# Added 1 to the hours played so that we have min. confidence for games bought but not played.\n",
    "user_game_interactions[user_idx, game_idx] = hours + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Validation\n",
    "To examine the effectiveness of the recommender system, I used top-k precision as my evaluation metric (k = 5, in this case). In order to implement this evaluation metric, I need to first identify users who have bought more than k games and then mask k preferences from the training set. This will bias the validation process towards users with higher number of purchases. However, it makes the problem easier to handle and cold start problem of recommender system is another topic that requires a lot more in depth analysis on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2189 users bought 10 or more games\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "# Count the number of purchases for each user\n",
    "purchase_counts = np.apply_along_axis(np.bincount, 1, user_game_pref.astype(int))\n",
    "buyers_idx = np.where(purchase_counts[:, 1] >= 2 * k)[0] #find the users who purchase 2 * k games\n",
    "print('{0} users bought {1} or more games'.format(len(buyers_idx), 2 * k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_frac = 0.2 # Let's save 10% of the data for validation and 10% for testing.\n",
    "test_users_idx = np.random.choice(buyers_idx,\n",
    "                                  size = int(np.ceil(len(buyers_idx) * test_frac)),\n",
    "                                  replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_users_idx = test_users_idx[:int(len(test_users_idx) / 2)]\n",
    "test_users_idx = test_users_idx[int(len(test_users_idx) / 2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# A function used to mask the preferences data from training matrix\n",
    "def data_process(dat, train, test, user_idx, k):\n",
    "    for user in user_idx:\n",
    "        purchases = np.where(dat[user, :] == 1)[0]\n",
    "        mask = np.random.choice(purchases, size = k, replace = False)\n",
    "        \n",
    "        train[user, mask] = 0\n",
    "        test[user, mask] = dat[user, mask]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_matrix = user_game_pref.copy()\n",
    "test_matrix = zero_matrix.copy()\n",
    "val_matrix = zero_matrix.copy()\n",
    "\n",
    "# Mask the train matrix and create the validation and test matrices\n",
    "train_matrix, val_matrix = data_process(user_game_pref, train_matrix, val_matrix, val_users_idx, k)\n",
    "train_matrix, test_matrix = data_process(user_game_pref, train_matrix, test_matrix, test_users_idx, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at what was actually accomplised\n",
    "# You can see the test matrix preferences are masked in the train matrix\n",
    "test_matrix[test_users_idx[0], test_matrix[test_users_idx[0], :].nonzero()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix[test_users_idx[0], test_matrix[test_users_idx[0], :].nonzero()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tensorflow Implementation\n",
    "I implemented the implicit CF in tensorflow because I want to get more familiar with it. You can do this with only scipy and numpy. In tensorflow, you have to define the computation graph first and then actually carry out the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Create a new graphs\n",
    "\n",
    "pref = tf.placeholder(tf.float32, (n_users, n_games))  # Here's the preference matrix\n",
    "interactions = tf.placeholder(tf.float32, (n_users, n_games)) # Here's the hours played matrix\n",
    "users_idx = tf.placeholder(tf.int32, (None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instead of directly multiplying the hours played matrix with the preference matrix, we want to add a confidence parameter. We can think of it as how much weight we should give to these interactions. The [original paper](http://yifanhu.net/PUB/cf.pdf) recommends setting the parameter to 40, but I found the result to be less than ideal. Instead, I sampled it from a uniform distribution and use gradient descent to find the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_features = 30 # Number of latent features to be extracted\n",
    "\n",
    "# The X matrix represents the user latent preferences with a shape of user x latent features\n",
    "X = tf.Variable(tf.truncated_normal([n_users, n_features], mean = 0, stddev = 0.05))\n",
    "\n",
    "# The Y matrix represents the game latent features with a shape of game x latent features\n",
    "Y = tf.Variable(tf.truncated_normal([n_games, n_features], mean = 0, stddev = 0.05))\n",
    "\n",
    "# Here's the initilization of the confidence parameter\n",
    "conf_alpha = tf.Variable(tf.random_uniform([1], 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## User and Item Bias\n",
    "Two other model parameters that I did not mention above are user and item biases. Intuitively, we would expect some users play games at a faster pace than the others. Similarly, we also expect some games would take less time to complete. User and item biases allow us to express these intuitions in a statistical model where we expect there are systematic differences in how users interact with the games and how games are played.\n",
    "\n",
    "One thing to note is that both biases are not necessary to build the recommender system, however, I found including these parameters improved the recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize a user bias vector\n",
    "user_bias = tf.Variable(tf.truncated_normal([n_users, 1], stddev = 0.2))\n",
    "\n",
    "# Concatenate the vector to the user matrix\n",
    "# Due to how matrix algebra works, we also need to add a column of ones to make sure\n",
    "# the resulting calculation will take into account the item biases.\n",
    "X_plus_bias = tf.concat([X, \n",
    "                         #tf.convert_to_tensor(user_bias, dtype = tf.float32),\n",
    "                         user_bias,\n",
    "                         tf.ones((n_users, 1), dtype = tf.float32)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize the item bias vector\n",
    "item_bias = tf.Variable(tf.truncated_normal([n_games, 1], stddev = 0.2))\n",
    "\n",
    "# Cocatenate the vector to the game matrix\n",
    "# Also, adds a column one for the same reason stated above.\n",
    "Y_plus_bias = tf.concat([Y, \n",
    "                         tf.ones((n_games, 1), dtype = tf.float32),\n",
    "                         item_bias],\n",
    "                         axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here, we finally multiply the matrices together to estimate the predicted preferences\n",
    "pred_pref = tf.matmul(X_plus_bias, Y_plus_bias, transpose_b=True)\n",
    "\n",
    "# Construct the confidence matrix with the hours played and alpha paramter\n",
    "conf = 1 + conf_alpha * interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The cost of the model would be the squared sum of predicted preferences and actual preferences. This cost is modified by the conference matrix. Finally, l2-regularization is also added to avoid overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.multiply(conf, tf.square(tf.subtract(pref, pred_pref))))\n",
    "l2_sqr = tf.nn.l2_loss(X) + tf.nn.l2_loss(Y) + tf.nn.l2_loss(user_bias) + tf.nn.l2_loss(item_bias)\n",
    "lambda_c = 0.01\n",
    "loss = cost + lambda_c * l2_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "optimize = tf.train.AdagradOptimizer(learning_rate = lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is a function that helps to calculate the top k precision \n",
    "def top_k_precision(pred, mat, k, user_idx):\n",
    "    precisions = []\n",
    "    \n",
    "    for user in user_idx:\n",
    "        rec = np.argsort(-pred[user, :]) # Found the top recommendation from the predictions\n",
    "        \n",
    "        top_k = rec[:k]\n",
    "        labels = mat[user, :].nonzero()[0]\n",
    "        \n",
    "        precision = len(set(top_k) & set(labels)) / float(k) # Calculate the precisions from actual labels\n",
    "        precisions.append(precision)\n",
    "    return np.mean(precisions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's the actual training. I calculate the validation precision after every 10 iterations. With higher number of iteration seems to overfit so I settled with 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 0... Training Loss 4042234.50... Train Precision 0.092... Val Precision 0.016\n",
      "Iterations 10... Training Loss 314923.16... Train Precision 0.422... Val Precision 0.038\n",
      "Iterations 20... Training Loss 247823.33... Train Precision 0.505... Val Precision 0.037\n",
      "Iterations 30... Training Loss 220597.56... Train Precision 0.556... Val Precision 0.040\n",
      "Iterations 40... Training Loss 202843.05... Train Precision 0.582... Val Precision 0.041\n",
      "Iterations 50... Training Loss 189578.31... Train Precision 0.598... Val Precision 0.045\n",
      "Iterations 60... Training Loss 178838.38... Train Precision 0.614... Val Precision 0.046\n",
      "Iterations 70... Training Loss 169620.34... Train Precision 0.632... Val Precision 0.044\n",
      "\n",
      "\n",
      "Test Precision0.061\n"
     ]
    }
   ],
   "source": [
    "iterations = 80\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        sess.run(optimize, feed_dict = {pref: train_matrix,\n",
    "                                        interactions: user_game_interactions})\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            mod_loss = sess.run(loss, feed_dict = {pref: train_matrix,\n",
    "                                                   interactions: user_game_interactions})            \n",
    "            mod_pred = pred_pref.eval()\n",
    "            train_precision = top_k_precision(mod_pred, train_matrix, k, val_users_idx)\n",
    "            val_precision = top_k_precision(mod_pred, val_matrix, k, val_users_idx)\n",
    "            print('Iterations {0}...'.format(i),\n",
    "                  'Training Loss {:.2f}...'.format(mod_loss),\n",
    "                  'Train Precision {:.3f}...'.format(train_precision),\n",
    "                  'Val Precision {:.3f}'.format(val_precision)\n",
    "                )\n",
    "\n",
    "    rec = pred_pref.eval()\n",
    "    test_precision = top_k_precision(rec, test_matrix, k, test_users_idx)\n",
    "    print('\\n')\n",
    "    print('Test Precision{:.3f}'.format(test_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The test top-k precision is not that high but I did not spend a lot of time optimizing the hyperparameters so there is probably a lot more room for improvement. Something for you to try if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Examples\n",
    "Below I print out a few examples of recommendations accompanied with the actual purchases of the users. Some recommendations made more sense than the others but overall precision is fairly low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "users = np.random.choice(test_users_idx, size = n_examples, replace = False)\n",
    "rec_games = np.argsort(-rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Games for 51557405 are ...\n",
      "We recommend these games\n",
      "Saints Row The Third, Fallout 4, Just Cause 2, Fallout New Vegas, Batman Arkham Asylum GOTY Edition\n",
      "\n",
      "\n",
      "The games that the user actually purchased are ...\n",
      "Alan Wake, Company of Heroes, Far Cry 3 Blood Dragon, Wolfenstein The Old Blood , NBA 2K14\n",
      "\n",
      "\n",
      "Precision of 0.0\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "Recommended Games for 165239864 are ...\n",
      "We recommend these games\n",
      "Counter-Strike, Grand Theft Auto V, DARK SOULS II, Fallout 4, Call of Duty Modern Warfare 2 - Multiplayer\n",
      "\n",
      "\n",
      "The games that the user actually purchased are ...\n",
      "Thief, DiRT 3, DiRT 3 Complete Edition, Valkyria Chronicles, Valkyria Chronicles Challenge of the Edy Detachment\n",
      "\n",
      "\n",
      "Precision of 0.0\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "Recommended Games for 102270213 are ...\n",
      "We recommend these games\n",
      "The Binding of Isaac, Garry's Mod, Path of Exile, Don't Starve, Clicker Heroes\n",
      "\n",
      "\n",
      "The games that the user actually purchased are ...\n",
      "The Mighty Quest For Epic Loot, Call of Duty Black Ops II - Multiplayer, Path of Exile, Warframe, TERA\n",
      "\n",
      "\n",
      "Precision of 0.2\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "Recommended Games for 95911580 are ...\n",
      "We recommend these games\n",
      "Empire Total War, Rust, War Thunder, Mount & Blade Warband, Football Manager 2014\n",
      "\n",
      "\n",
      "The games that the user actually purchased are ...\n",
      "Hotline Miami, Total War ROME II - Emperor Edition, Hearts of Iron III, Hearts of Iron III German II Sprite Pack, Europa Universalis IV Conquistadors Unit pack \n",
      "\n",
      "\n",
      "Precision of 0.0\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "Recommended Games for 188189790 are ...\n",
      "We recommend these games\n",
      "Dota 2, Team Fortress 2, Warframe, Robocraft, Heroes & Generals\n",
      "\n",
      "\n",
      "The games that the user actually purchased are ...\n",
      "Dota 2, The Elder Scrolls V Skyrim, Warframe, Cubic Castles, Super Crate Box\n",
      "\n",
      "\n",
      "Precision of 0.4\n",
      "--------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for user in users:\n",
    "    print('Recommended Games for {0} are ...'.format(idx2user[user]))\n",
    "    purchase_history = np.where(train_matrix[user, :] != 0)[0]\n",
    "    recommendations = rec_games[user, :]\n",
    "\n",
    "    \n",
    "    new_recommendations = recommendations[~np.in1d(recommendations, purchase_history)][:k]\n",
    "    \n",
    "    print('We recommend these games')\n",
    "    print(', '.join([idx2game[game] for game in new_recommendations]))\n",
    "    print('\\n')\n",
    "    print('The games that the user actually purchased are ...')\n",
    "    print(', '.join([idx2game[game] for game in np.where(test_matrix[user, :] != 0)[0]]))\n",
    "    print('\\n')\n",
    "    print('Precision of {0}'.format(len(set(new_recommendations) & set(np.where(test_matrix[user, :] != 0)[0])) / float(k)))\n",
    "    print('--------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Final Words\n",
    "Beside optimizing the hyperparameters, I did not spend a lot of time cleaning the data. Some of the games are more like DLCs and variations of the game. Consolidating some of these games may yield better results. \n",
    "\n",
    "Like I said, there are actually packages out there that can build the implicit CF for you more efficiently, but I find it to be informative to implement the algorithm on my own and I would recommend anyone who is learning the topic to do so as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
